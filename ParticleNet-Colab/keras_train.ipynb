{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted from Particle Net Code\n",
    "\n",
    "https://github.com/hqucms/ParticleNet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Particle Net Paper\n",
    "\n",
    "ParticleNet: Jet Tagging via Particle Clouds: \n",
    "\n",
    "https://arxiv.org/abs/1902.08570\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install modules not in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awkward0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount google drive\n",
    "\n",
    "The converted (.h5 to .awkd) files should still be in your google drive, but you may have to mount your google drive again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_keras_model.py\n",
    "\n",
    "Functions defining the architecture of the networks. For convenience with google colab the code is copy/pasted to the keras_train.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "\n",
    "# A shape is (N, P_A, C), B shape is (N, P_B, C)\n",
    "# D shape is (N, P_A, P_B)\n",
    "def batch_distance_matrix_general(A, B):\n",
    "    with tf.name_scope('dmat'):\n",
    "        r_A = tf.reduce_sum(A * A, axis=2, keepdims=True)\n",
    "        r_B = tf.reduce_sum(B * B, axis=2, keepdims=True)\n",
    "        m = tf.matmul(A, tf.transpose(B, perm=(0, 2, 1)))\n",
    "        D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 2, 1))\n",
    "        return D\n",
    "\n",
    "\n",
    "def knn(num_points, k, topk_indices, features):\n",
    "    # topk_indices: (N, P, K)\n",
    "    # features: (N, P, C)\n",
    "    with tf.name_scope('knn'):\n",
    "        queries_shape = tf.shape(features)\n",
    "        batch_size = queries_shape[0]\n",
    "        batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, num_points, k, 1))\n",
    "        indices = tf.concat([batch_indices, tf.expand_dims(topk_indices, axis=3)], axis=3)  # (N, P, K, 2)\n",
    "        return tf.gather_nd(features, indices)\n",
    "\n",
    "\n",
    "def edge_conv(points, features, num_points, K, channels, with_bn=True, activation='relu', pooling='average', name='edgeconv'):\n",
    "    \"\"\"EdgeConv\n",
    "    Args:\n",
    "        K: int, number of neighbors\n",
    "        in_channels: # of input channels\n",
    "        channels: tuple of output channels\n",
    "        pooling: pooling method ('max' or 'average')\n",
    "    Inputs:\n",
    "        points: (N, P, C_p)\n",
    "        features: (N, P, C_0)\n",
    "    Returns:\n",
    "        transformed points: (N, P, C_out), C_out = channels[-1]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('edgeconv'):\n",
    "\n",
    "        # distance\n",
    "        D = batch_distance_matrix_general(points, points)  # (N, P, P)\n",
    "        _, indices = tf.nn.top_k(-D, k=K + 1)  # (N, P, K+1)\n",
    "        indices = indices[:, :, 1:]  # (N, P, K)\n",
    "\n",
    "        fts = features\n",
    "        knn_fts = knn(num_points, K, indices, fts)  # (N, P, K, C)\n",
    "        knn_fts_center = tf.tile(tf.expand_dims(fts, axis=2), (1, 1, K, 1))  # (N, P, K, C)\n",
    "        knn_fts = tf.concat([knn_fts_center, tf.subtract(knn_fts, knn_fts_center)], axis=-1)  # (N, P, K, 2*C)\n",
    "\n",
    "        x = knn_fts\n",
    "        for idx, channel in enumerate(channels):\n",
    "            x = keras.layers.Conv2D(channel, kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "                                    use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_conv%d' % (name, idx))(x)\n",
    "            if with_bn:\n",
    "                x = keras.layers.BatchNormalization(name='%s_bn%d' % (name, idx))(x)\n",
    "            if activation:\n",
    "                x = keras.layers.Activation(activation, name='%s_act%d' % (name, idx))(x)\n",
    "\n",
    "        if pooling == 'max':\n",
    "            fts = tf.reduce_max(x, axis=2)  # (N, P, C')\n",
    "        else:\n",
    "            fts = tf.reduce_mean(x, axis=2)  # (N, P, C')\n",
    "\n",
    "        # shortcut\n",
    "        sc = keras.layers.Conv2D(channels[-1], kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "                                 use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_sc_conv' % name)(tf.expand_dims(features, axis=2))\n",
    "        if with_bn:\n",
    "            sc = keras.layers.BatchNormalization(name='%s_sc_bn' % name)(sc)\n",
    "        sc = tf.squeeze(sc, axis=2)\n",
    "\n",
    "        if activation:\n",
    "            return keras.layers.Activation(activation, name='%s_sc_act' % name)(sc + fts)  # (N, P, C')\n",
    "        else:\n",
    "            return sc + fts\n",
    "\n",
    "\n",
    "def _particle_net_base(points, features=None, mask=None, setting=None, name='particle_net'):\n",
    "    # points : (N, P, C_coord)\n",
    "    # features:  (N, P, C_features), optional\n",
    "    # mask: (N, P, 1), optinal\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        if features is None:\n",
    "            features = points\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(tf.not_equal(mask, 0), dtype='float32')  # 1 if valid\n",
    "            coord_shift = tf.multiply(999., tf.cast(tf.equal(mask, 0), dtype='float32'))  # make non-valid positions to 99\n",
    "\n",
    "        fts = tf.squeeze(keras.layers.BatchNormalization(name='%s_fts_bn' % name)(tf.expand_dims(features, axis=2)), axis=2)\n",
    "        for layer_idx, layer_param in enumerate(setting.conv_params):\n",
    "            K, channels = layer_param\n",
    "            pts = tf.add(coord_shift, points) if layer_idx == 0 else tf.add(coord_shift, fts)\n",
    "            fts = edge_conv(pts, fts, setting.num_points, K, channels, with_bn=True, activation='relu',\n",
    "                            pooling=setting.conv_pooling, name='%s_%s%d' % (name, 'EdgeConv', layer_idx))\n",
    "\n",
    "        if mask is not None:\n",
    "            fts = tf.multiply(fts, mask)\n",
    "\n",
    "        pool = tf.reduce_mean(fts, axis=1)  # (N, C)\n",
    "\n",
    "        if setting.fc_params is not None:\n",
    "            x = pool\n",
    "            for layer_idx, layer_param in enumerate(setting.fc_params):\n",
    "                units, drop_rate = layer_param\n",
    "                x = keras.layers.Dense(units, activation='relu')(x)\n",
    "                if drop_rate is not None and drop_rate > 0:\n",
    "                    x = keras.layers.Dropout(drop_rate)(x)\n",
    "            out = keras.layers.Dense(setting.num_class, activation='softmax')(x)\n",
    "            return out  # (N, num_classes)\n",
    "        else:\n",
    "            return pool\n",
    "\n",
    "\n",
    "class _DotDict:\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_particle_net(num_classes, input_shapes):\n",
    "    r\"\"\"ParticleNet model from `\"ParticleNet: Jet Tagging via Particle Clouds\"\n",
    "    <https://arxiv.org/abs/1902.08570>`_ paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    input_shapes : dict\n",
    "        The shapes of each input (`points`, `features`, `mask`).\n",
    "    \"\"\"\n",
    "    setting = _DotDict()\n",
    "    setting.num_class = num_classes\n",
    "    # conv_params: list of tuple in the format (K, (C1, C2, C3))\n",
    "    setting.conv_params = [\n",
    "        (16, (64, 64, 64)),\n",
    "        (16, (128, 128, 128)),\n",
    "        (16, (256, 256, 256)),\n",
    "        ]\n",
    "    # conv_pooling: 'average' or 'max'\n",
    "    setting.conv_pooling = 'average'\n",
    "    # fc_params: list of tuples in the format (C, drop_rate)\n",
    "    setting.fc_params = [(256, 0.1)]\n",
    "    setting.num_points = input_shapes['points'][0]\n",
    "\n",
    "    points = keras.Input(name='points', shape=input_shapes['points'])\n",
    "    features = keras.Input(name='features', shape=input_shapes['features']) if 'features' in input_shapes else None\n",
    "    mask = keras.Input(name='mask', shape=input_shapes['mask']) if 'mask' in input_shapes else None\n",
    "    outputs = _particle_net_base(points, features, mask, setting, name='ParticleNet')\n",
    "\n",
    "    return keras.Model(inputs=[points, features, mask], outputs=outputs, name='ParticleNet')\n",
    "\n",
    "\n",
    "def get_particle_net_lite(num_classes, input_shapes):\n",
    "    r\"\"\"ParticleNet-Lite model from `\"ParticleNet: Jet Tagging via Particle Clouds\"\n",
    "    <https://arxiv.org/abs/1902.08570>`_ paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    input_shapes : dict\n",
    "        The shapes of each input (`points`, `features`, `mask`).\n",
    "    \"\"\"\n",
    "    setting = _DotDict()\n",
    "    setting.num_class = num_classes\n",
    "    # conv_params: list of tuple in the format (K, (C1, C2, C3))\n",
    "    setting.conv_params = [\n",
    "        (7, (32, 32, 32)),\n",
    "        (7, (64, 64, 64)),\n",
    "        ]\n",
    "    # conv_pooling: 'average' or 'max'\n",
    "    setting.conv_pooling = 'average'\n",
    "    # fc_params: list of tuples in the format (C, drop_rate)\n",
    "    setting.fc_params = [(128, 0.1)]\n",
    "    setting.num_points = input_shapes['points'][0]\n",
    "\n",
    "    points = keras.Input(name='points', shape=input_shapes['points'])\n",
    "    features = keras.Input(name='features', shape=input_shapes['features']) if 'features' in input_shapes else None\n",
    "    mask = keras.Input(name='mask', shape=input_shapes['mask']) if 'mask' in input_shapes else None\n",
    "    outputs = _particle_net_base(points, features, mask, setting, name='ParticleNet')\n",
    "\n",
    "    return keras.Model(inputs=[points, features, mask], outputs=outputs, name='ParticleNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin keras_train notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward0 as awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_arrays(a, keys, axis=-1):\n",
    "    flat_arr = np.stack([a[k].flatten() for k in keys], axis=axis)\n",
    "    return awkward.JaggedArray.fromcounts(a[keys[0]].counts, flat_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(a, maxlen, value=0., dtype='float32'):\n",
    "    x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "    for idx, s in enumerate(a):\n",
    "        if not len(s):\n",
    "            continue\n",
    "        trunc = s[:maxlen].astype(dtype)\n",
    "        x[idx, :len(trunc)] = trunc\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, filepath, feature_dict = {}, label='label', pad_len=100, data_format='channel_first'):\n",
    "        self.filepath = filepath\n",
    "        self.feature_dict = feature_dict\n",
    "        if len(feature_dict)==0:\n",
    "            feature_dict['points'] = ['part_etarel', 'part_phirel']\n",
    "            feature_dict['features'] = ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel']\n",
    "            feature_dict['mask'] = ['part_pt_log']\n",
    "        self.label = label\n",
    "        self.pad_len = pad_len\n",
    "        assert data_format in ('channel_first', 'channel_last')\n",
    "        self.stack_axis = 1 if data_format=='channel_first' else -1\n",
    "        self._values = {}\n",
    "        self._label = None\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        logging.info('Start loading file %s' % self.filepath)\n",
    "        counts = None\n",
    "        with awkward.load(self.filepath) as a:\n",
    "            self._label = a[self.label]\n",
    "            for k in self.feature_dict:\n",
    "                cols = self.feature_dict[k]\n",
    "                if not isinstance(cols, (list, tuple)):\n",
    "                    cols = [cols]\n",
    "                arrs = []\n",
    "                for col in cols:\n",
    "                    if counts is None:\n",
    "                        counts = a[col].counts\n",
    "                    else:\n",
    "                        assert np.array_equal(counts, a[col].counts)\n",
    "                    arrs.append(pad_array(a[col], self.pad_len))\n",
    "                self._values[k] = np.stack(arrs, axis=self.stack_axis)\n",
    "        logging.info('Finished loading file %s' % self.filepath)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._label)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key==self.label:\n",
    "            return self._label\n",
    "        else:\n",
    "            return self._values[key]\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._values\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._label\n",
    "\n",
    "    def shuffle(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        shuffle_indices = np.arange(self.__len__())\n",
    "        np.random.shuffle(shuffle_indices)\n",
    "        for k in self._values:\n",
    "            self._values[k] = self._values[k][shuffle_indices]\n",
    "        self._label = self._label[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the converted awkward files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('/content/drive/MyDrive/converted/train_file_0.awkd', data_format='channel_last')\n",
    "val_dataset = Dataset('/content/drive/MyDrive/converted/val_file_0.awkd', data_format='channel_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_type = 'particle_net_lite' # choose between 'particle_net' and 'particle_net_lite'\n",
    "num_classes = train_dataset.y.shape[1]\n",
    "input_shapes = {k:train_dataset[k].shape[1:] for k in train_dataset.X}\n",
    "if 'lite' in model_type:\n",
    "    model = get_particle_net_lite(num_classes, input_shapes)\n",
    "else:\n",
    "    model = get_particle_net(num_classes, input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 1024 if 'lite' in model_type else 384\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 10:\n",
    "        lr *= 0.1\n",
    "    elif epoch > 20:\n",
    "        lr *= 0.01\n",
    "    logging.info('Learning rate: %f'%lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model model saving directory.\n",
    "import os\n",
    "save_dir = 'model_checkpoints'\n",
    "model_name = '%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "progress_bar = keras.callbacks.ProgbarLogger()\n",
    "callbacks = [checkpoint, lr_scheduler, progress_bar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_dataset.shuffle()\n",
    "model.fit(train_dataset.X, train_dataset.y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          epochs=1, # --- train only for 1 epoch here for demonstration ---\n",
    "          validation_data=(val_dataset.X, val_dataset.y),\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
