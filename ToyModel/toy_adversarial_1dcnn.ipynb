{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Regularization for 1D CNN Toy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwZNOAMZcxl3"
   },
   "source": [
    "#### Adapted from tensorflow Adversarial Regularization for Image Classification tutorial:\n",
    "\n",
    "https://www.tensorflow.org/neural_structured_learning/tutorials/adversarial_keras_cnn_mnist\n",
    "\n",
    "[github](https://github.com/tensorflow/neural-structured-learning/blob/master/g3doc/tutorials/adversarial_keras_cnn_mnist.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgSOF-49Q7kS"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RhmgQ7-mlrl"
   },
   "source": [
    "Install the Neural Structured Learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByJ7133BQULR"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet neural-structured-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuqEuAYzTMo0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import neural_structured_learning as nsl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow Version {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LwBtQGaTvbe"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "\n",
    "Hyperparamaters explanations from https://www.tensorflow.org/neural_structured_learning/tutorials/adversarial_keras_cnn_mnist\n",
    "\n",
    "\n",
    "Input/Output:\n",
    "\n",
    "*   **`input_shape`**: The shape of the input tensor. Each image is 28-by-28\n",
    "pixels with 1 channel.\n",
    "*   **`num_classes`**: There are a total of 10 classes, corresponding to 10\n",
    "digits [0-9].\n",
    "\n",
    "Model architecture:\n",
    "\n",
    "*   **`conv_filters`**: A list of numbers, each specifying the number of\n",
    "filters in a convolutional layer.\n",
    "*   **`kernel_size`**: The size of 2D convolution window, shared by all\n",
    "convolutional layers.\n",
    "*   **`pool_size`**: Factors to downscale the image in each max-pooling layer.\n",
    "*   **`num_fc_units`**: The number of units (i.e., width) of each\n",
    "fully-connected layer.\n",
    "\n",
    "Training and evaluation:\n",
    "\n",
    "*  **`batch_size`**: Batch size used for training and evaluation.\n",
    "*  **`epochs`**: The number of training epochs.\n",
    "\n",
    "Adversarial learning:\n",
    "\n",
    "*   **`adv_multiplier`**: The weight of adversarial loss in the training\n",
    "objective, relative to the labeled loss.\n",
    "*   **`adv_step_size`**: The magnitude of adversarial perturbation.\n",
    "*  **`adv_grad_norm`**: The norm to measure the magnitude of adversarial\n",
    "perturbation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOc8YdmIRSHo"
   },
   "outputs": [],
   "source": [
    "# this notebook does note use any hyperparameters to make the base model\n",
    "\n",
    "class HParams(object):\n",
    "    def __init__(self):\n",
    "#         self.input_shape = [10, 1]\n",
    "#         self.num_classes = 10\n",
    "#         self.conv_filters = [64, 3]\n",
    "#         self.kernel_size = (3)\n",
    "#         self.pool_size = (2)\n",
    "#         self.num_fc_units = [10]\n",
    "#         self.batch_size = 20\n",
    "        self.epochs = 10\n",
    "        self.adv_multiplier = 1.0\n",
    "        self.adv_step_size = 2\n",
    "        self.adv_grad_norm = 'infinity'\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(array):\n",
    "    return np.array(array[-1], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1dK6E4axNHB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FEATURE_INPUT_NAME = ['pt', 'eta', 'phi', 'mass', 'thetalab', 'radiilab', 'zlab']\n",
    "FEATURE_INPUT_NAME = ['pt', 'thetalab']\n",
    "LABEL_INPUT_NAME = 'labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Perturbed and Un-perturbed samples\n",
    "\n",
    "npz files are output of `UBCMS-XAI/AdversarialNetworks/ToyModel/makeFourVectors.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbed samples\n",
    "perturbed_data_train = np.load('data/jetConstTrain_overlap_perturb.npz')\n",
    "perturbed_data_test = np.load('data/jetConstTest_overlap_perturb.npz')\n",
    "perturbed_batch = len(perturbed_data_train['pt'])\n",
    "perturbed_feat_all = [key for key in perturbed_data_train.keys()]\n",
    "perturbed_feat_all.remove('labels')\n",
    "print('Perturbed dataset features:')\n",
    "print(perturbed_feat_all)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-perturbed samples\n",
    "data_train = np.load('data/jetConstTrain_overlap.npz')\n",
    "data_test = np.load('data/jetConstTest_overlap.npz')\n",
    "batch = len(data_train['pt'])\n",
    "feat_all = [key for key in data_train.keys()]\n",
    "feat_all.remove('labels')\n",
    "print('Un-perturbed dataset features:')\n",
    "print(feat_all)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create perturbed dataset dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_data_train_dict = {}\n",
    "perturbed_data_test_dict  = {}\n",
    "\n",
    "for key in perturbed_data_train.keys():\n",
    "    if not 'labels' in key:\n",
    "        perturbed_data_train_dict[key] = perturbed_data_train[key].reshape(20000, 10, 1)\n",
    "        perturbed_data_test_dict[key] = perturbed_data_test[key].reshape(20000, 10, 1)\n",
    "\n",
    "perturbed_data_train_dict['labels'] = perturbed_data_train['labels'][:,1]\n",
    "perturbed_data_test_dict['labels'] = perturbed_data_test['labels'][:,1]\n",
    "\n",
    "\n",
    "perturbed_data_train_list = [perturbed_data_train_dict[key] for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]]\n",
    "perturbed_data_test_list  = [perturbed_data_test_dict[key]  for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create un-perturbed dataset dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train_dict = {}\n",
    "data_test_dict  = {}\n",
    "\n",
    "for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]:\n",
    "    if not 'labels' in key:\n",
    "        data_train_dict[key] = data_train[key].reshape(20000, 10, 1)\n",
    "        data_test_dict[key] = data_test[key].reshape(20000, 10, 1)\n",
    "\n",
    "data_train_dict['labels'] = data_train['labels'][:,1]\n",
    "data_test_dict['labels'] = data_test['labels'][:,1]\n",
    "        \n",
    "\n",
    "data_train_dict_subset = {key: data_train_dict[key] for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]}\n",
    "data_test_dict_subset  = {key: data_test_dict[key]  for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]}\n",
    "\n",
    "data_train_list = [data_train_dict[key] for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]]\n",
    "data_test_list  = [data_test_dict[key]  for key in FEATURE_INPUT_NAME+[LABEL_INPUT_NAME]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig_true = (getLabels(data_train_list) == 1)\n",
    "bkg_true = (getLabels(data_train_list) == 0)\n",
    "sig_pert = (getLabels(perturbed_data_train_list) == 1)\n",
    "\n",
    "for i, (feature, perturbed_feature) in enumerate(zip(data_train_list, perturbed_data_train_list)):\n",
    "    \n",
    "    if(i == len(perturbed_data_train_list) - 1): continue\n",
    "\n",
    "    bmax = np.max(feature.flatten())\n",
    "    bmin = np.min(feature.flatten())\n",
    "    \n",
    "    plt.hist(feature[sig_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5, label='Signal')\n",
    "    plt.hist(feature[bkg_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5, label='Background')\n",
    "    plt.hist(perturbed_feature[sig_pert].flatten(), bins=np.linspace(bmin, bmax, 50), color='k', histtype='step', label='Perturbed Signal')\n",
    "    plt.xlabel(FEATURE_INPUT_NAME[i])\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrrMpPNmpCKK"
   },
   "source": [
    "## Base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model():\n",
    "\n",
    "\n",
    "    input_shape = (10, 1)\n",
    "    inputs = [tf.keras.Input(shape=input_shape, dtype=tf.float32, name=name) for name in FEATURE_INPUT_NAME]\n",
    "    print(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.concatenate(inputs = [*inputs], axis=-1, name = 'concat')\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(64, 3, padding = 'same', activation='relu', name = 'conv1')(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 1, padding = 'same', activation='relu', name = 'conv2')(x)\n",
    "    x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(32, 3, padding = 'same', activation='relu', name = 'conv3')(x)\n",
    "    x = tf.keras.layers.Conv1D(32, 1, padding = 'same', activation='relu', name = 'conv4')(x)\n",
    "    x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "    x = tf.keras.layers.Flatten(name = 'flatten')(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name = 'relu')(x)\n",
    "    pred = tf.keras.layers.Dense(2, activation='softmax', name = 'output')(x) \n",
    "    model = tf.keras.Model(inputs=[*inputs], outputs=pred)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "288nsmN5pLoo"
   },
   "outputs": [],
   "source": [
    "base_model = build_base_model()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlTUGn1t_HAr"
   },
   "source": [
    "### Train and evaluate the model on perturbed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2cFDbmRpRMp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "\n",
    "base_model.fit(perturbed_data_train_list[:-1], getLabels(perturbed_data_train_list), epochs=HPARAMS.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J94Y_WTaqAsi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "results = base_model.evaluate(perturbed_data_test_list[:-1], getLabels(perturbed_data_test_list))\n",
    "named_results = dict(zip(base_model.metrics_names, results))\n",
    "print('\\naccuracy:', named_results['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CemXA8N9q336"
   },
   "source": [
    "## Adversarial-regularized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TObqJLEX4sQq"
   },
   "outputs": [],
   "source": [
    "base_adv_model = build_base_model()\n",
    "adv_model = nsl.keras.AdversarialRegularization(\n",
    "    base_adv_model,\n",
    "    label_keys = ['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTSK-cHbuWDw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adv_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "\n",
    "\n",
    "adv_model.fit(perturbed_data_train_dict, epochs=HPARAMS.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v_Jn7wuviZx"
   },
   "outputs": [],
   "source": [
    "results = adv_model.evaluate(perturbed_data_test_dict)\n",
    "named_results = dict(zip(adv_model.metrics_names, results))\n",
    "print('\\naccuracy:', named_results['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXK9MGG8lBX3"
   },
   "source": [
    "## Evaluate models with un-perturbed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_config= nsl.configs.make_adv_reg_config(\n",
    "    multiplier=HPARAMS.adv_multiplier,\n",
    "    adv_step_size=HPARAMS.adv_step_size,\n",
    "    adv_grad_norm=HPARAMS.adv_grad_norm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLkYw54pvxJO"
   },
   "outputs": [],
   "source": [
    "reference_model = nsl.keras.AdversarialRegularization(\n",
    "    base_model,\n",
    "    label_keys = ['labels'],\n",
    "    adv_config=adv_config\n",
    ")\n",
    "reference_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igRBxPlPm_JE"
   },
   "outputs": [],
   "source": [
    "models_to_eval = {\n",
    "    'base': base_model,\n",
    "    'adv-regularized': adv_model.base_model\n",
    "}\n",
    "metrics = {\n",
    "    name: tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    for name in models_to_eval.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_cut = 0.6\n",
    "predictions = []\n",
    "predictions.append({})\n",
    "\n",
    "\n",
    "perturbed_batch = [pbatch[:200] for pbatch in perturbed_data_test_list[:-1]]\n",
    "\n",
    "sample_batch = [sbatch[:200] for sbatch in data_test_list[:-1]]\n",
    "\n",
    "\n",
    "y_true = np.array(perturbed_batch)\n",
    "    \n",
    "for name, model in models_to_eval.items():\n",
    "    print('evaluating {0} model'.format(name))\n",
    "    y_pred = model(data_test_dict)\n",
    "\n",
    "    metrics[name](y_true, y_pred)\n",
    "    \n",
    "    y = np.where(y_pred[:,1] > pred_cut, 1, 0)\n",
    "    predictions[-1][name] = y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iK9vO_xKJfg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_index = slice(0,200)\n",
    "    \n",
    "n_col = 3\n",
    "n_row = 1\n",
    "\n",
    "labels =  getLabels(data_test_list)\n",
    "\n",
    "sig_true = (labels == 1)\n",
    "bkg_true = (labels == 0)\n",
    "\n",
    "\n",
    "\n",
    "sig_base = (predictions[0]['base'] == 1)\n",
    "bkg_base = (predictions[0]['base'] == 0)\n",
    "\n",
    "sig_adv = (predictions[0]['adv-regularized'] == 1)\n",
    "bkg_adv = (predictions[0]['adv-regularized'] == 0)\n",
    "\n",
    "for i, feature in enumerate(data_test_list):\n",
    "    if(i==len(data_test_list)-1):continue\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(n_row, n_col, figsize=(12, 4) )\n",
    "    bmax = np.max(feature.flatten())\n",
    "    bmin = np.min(feature.flatten())\n",
    "    \n",
    "    ax[0].hist(feature[sig_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[0].hist(feature[bkg_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[0].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[0].set_ylabel('Counts')\n",
    "    ax[0].set_title('Labels')\n",
    "    ylim = ax[0].get_ylim()\n",
    "\n",
    "    ax[1].hist(feature[sig_base].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[1].hist(feature[bkg_base].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[1].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[1].set_ylabel('Counts')\n",
    "    ax[1].set_title('Base Model')\n",
    "    ax[1].set_ylim(ylim)\n",
    "    \n",
    "    ax[2].hist(feature[sig_adv].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[2].hist(feature[bkg_adv].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[2].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[2].set_ylabel('Counts')\n",
    "    ax[2].set_title('Adv Regularized Model')\n",
    "    ax[2].set_ylim(ylim)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctly Predicted Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(data_test_list):\n",
    "    if(i==len(data_test_list)-1):continue\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(n_row, n_col, figsize=(12, 4) )\n",
    "    bmax = np.max(feature.flatten())\n",
    "    bmin = np.min(feature.flatten())\n",
    "    \n",
    "    ax[0].hist(feature[sig_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[0].hist(feature[bkg_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[0].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[0].set_ylabel('Counts')\n",
    "    ax[0].set_title('Labels')\n",
    "    ylim = ax[0].get_ylim()\n",
    "\n",
    "    ax[1].hist(feature[sig_base & sig_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[1].hist(feature[bkg_base & bkg_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[1].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[1].set_ylabel('Counts')\n",
    "    ax[1].set_title('Base Model')\n",
    "    ax[1].set_ylim(ylim)\n",
    "    \n",
    "    ax[2].hist(feature[sig_adv & sig_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[2].hist(feature[bkg_adv & bkg_true].flatten(), bins=np.linspace(bmin, bmax, 50), alpha=0.5)\n",
    "    ax[2].set_xlabel(FEATURE_INPUT_NAME[i])\n",
    "    ax[2].set_ylabel('Counts')\n",
    "    ax[2].set_title('Adv Regularized Model')\n",
    "    ax[2].set_ylim(ylim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sig = len(sig_true[sig_true & sig_true])\n",
    "true_bkg = len(bkg_true[bkg_true & bkg_true])\n",
    "\n",
    "true_base_sig = len(sig_true[sig_true & sig_base])\n",
    "true_base_bkg = len(bkg_true[bkg_true & bkg_base])\n",
    "\n",
    "true_adv_sig = len(sig_true[sig_true & sig_adv])\n",
    "true_adv_bkg = len(bkg_true[bkg_true & bkg_adv])\n",
    "\n",
    "print()\n",
    "print('Total Labels', true_sig + true_bkg)\n",
    "print('Total Correctly Predicted Base Model', true_base_sig + true_base_bkg)\n",
    "print('Total Correctly Predicted Adv Model ', true_adv_sig + true_adv_bkg)\n",
    "print()\n",
    "print('True Signal', true_sig)\n",
    "print('Correctly Predicted Signal, Base Model', true_base_sig)\n",
    "print('Correctly Predicted Signal, Adv Model ', true_adv_sig)\n",
    "print()\n",
    "print('True Background', true_bkg)\n",
    "print('Correctly Predicted Background, Base Model', true_base_bkg)\n",
    "print('Correctly Predicted Background, Adv Model ', true_adv_bkg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Adversarial regularization for image classification",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
