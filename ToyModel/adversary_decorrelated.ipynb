{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import sklearn\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "from plottingFunctions import *\n",
    "from modelFunctions import *\n",
    "\n",
    "print('Tensorflow Version {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_notebook=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "    \n",
    "if not os.path.isdir(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self):\n",
    "        self.Lambda = 10\n",
    "        self.batchsize = 50\n",
    "        self.trainingBatchSize = 50\n",
    "        self.thetaBins = 6\n",
    "        self.trainingEpochs = 20\n",
    "        self.adversary_lr = 1e-3\n",
    "        self.classifier_lr = 1e-3\n",
    "        self.nUpdates_Classifier = 10\n",
    "        self.valSplit = 0.9\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Parameters\n",
    "\n",
    "```\n",
    "features = ['pt', 'eta', 'phi', 'mass', 'theta', 'radii', 'z', 'tau1', 'tau2', 'labels']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierOnly = False\n",
    "alpha = 0.5\n",
    "\n",
    "features = ['theta', 'radii', 'z', 'tau1', 'tau2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(inputs, concatenated_layers):\n",
    "    \n",
    "        \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(concatenated_layers)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    output = tf.keras.layers.Dense(2, activation='softmax', name = 'labels')(x) \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='Classifier')\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adversary(inputs, concatenated_layers):\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='Adversary_Dense64')(concatenated_layers)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='Adversary_Dense128')(x)\n",
    "\n",
    "    inputs = [input_from_model, feature_input]\n",
    "    output = tf.keras.layers.Dense(HPARAMS.thetaBins, activation='softmax', name = 'output')(x) \n",
    "        \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='Adversary')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets as dictionaries\n",
    "### Datasets generated with ToyModel/makeFourVectors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfilename = 'data/jetConstTrain_overlap.npz'\n",
    "testfilename = 'data/jetConstTest_overlap.npz'\n",
    "\n",
    "# created by running makeFourVectors.ipynb with addPerturbation = False\n",
    "data_train = np.load(trainfilename)\n",
    "data_test = np.load(testfilename)\n",
    "\n",
    "# created by running makeFourVectors.ipynb with addPerturbation = True\n",
    "data_train_shift = np.load(trainfilename.replace('.npz', '_perturb.npz'))\n",
    "data_test_shift = np.load(testfilename.replace('.npz', '_perturb.npz'))\n",
    "    \n",
    "feat_list = features\n",
    "feat_xaug = []\n",
    "\n",
    "Nlist = len(feat_list)\n",
    "Nxaug = len(feat_xaug)\n",
    "nEvents = len(data_train['pt'][:,0].flatten())\n",
    "\n",
    "print()\n",
    "print('Number of particle list features: ', Nlist)\n",
    "print('Number of XAUG features: ', Nxaug)\n",
    "print('N Events: ', nEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, X_val, Y_val = getXY(data_train,\n",
    "                                                       data_test,\n",
    "                                                       features,\n",
    "                                                       HPARAMS.valSplit,\n",
    "                                                       nEvents,\n",
    "                                                      )\n",
    "\n",
    "\n",
    "X_train_shift, Y_train_shift, X_test_shift, Y_test_shift, X_val_shift, Y_val_shift = getXY(data_train_shift,\n",
    "                                                                                           data_test_shift,\n",
    "                                                                                           features,\n",
    "                                                                                           HPARAMS.valSplit,\n",
    "                                                                                           nEvents,\n",
    "                                                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [key for key in X_train.keys() if not 'labels' in key]\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make $\\mathrm{log}(p_{T})$ and $\\mathrm{log}(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train['pt'] = np.log(X_train['pt'])\n",
    "    X_test['pt'] = np.log(X_test['pt'])\n",
    "    X_val['pt'] = np.log(X_val['pt'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    X_train['z'] = np.log(X_train['z'])\n",
    "    X_test['z'] = np.log(X_test['z'])\n",
    "    X_val['z'] = np.log(X_val['z'])\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotFeature(X_train, Y_train, 'theta', alpha)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotFeature(X_train, Y_train, 'z', alpha)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotFeature(X_train, Y_train, 'radii', alpha)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotFeature(X_train, Y_train, 'tau1', alpha)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotFeature(X_train, Y_train, 'tau2', alpha)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal difference adversary\n",
    "Note: This is not currently used as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'theta', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train_shift, Y_train_shift, 'theta', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adv, Y_adv = getAdversary(X_train['theta'], X_train_shift['theta'], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin $\\Delta \\theta$\n",
    "`binned_theta` is an input to the adversary model and\n",
    "`theta_labels` are the labels for the adversary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetabins = np.linspace(0, 0.3, HPARAMS.thetaBins)\n",
    "binned_theta = np.digitize(X_train['theta'], thetabins) - 1\n",
    "theta_labels = tf.one_hot(binned_theta.squeeze(), HPARAMS.thetaBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nfirst 5 bins\\n')\n",
    "print(binned_theta[:5])\n",
    "print('\\nfirst 5 labels\\n')\n",
    "print(theta_labels.numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train['theta'].flatten(), bins=thetabins, histtype='step', density=True)\n",
    "plt.xlabel(r'$\\Delta \\theta$')\n",
    "plt.ylabel('Density')\n",
    "plt.title(r'Binned $\\Delta \\theta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier and Adversary Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = getDataset(X_train, Y_train, HPARAMS.batchsize)\n",
    "valid_dataset = getDataset(X_val, Y_val, HPARAMS.batchsize)\n",
    "train_dataset_adv = getDataset(X_train['theta'], theta_labels, HPARAMS.batchsize)\n",
    "\n",
    "train_dataset_theta = [element[0]['theta'] for element in train_dataset]\n",
    "train_dataset_theta_Y = [element[1] for element in train_dataset_adv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of data to train and validate\n",
    "prop_train = 0.90 \n",
    "prop_validate = 1 - prop_train \n",
    "\n",
    "if prop_train == 0:\n",
    "    print('You need to train on data.')\n",
    "    prop_train = 0.50\n",
    "    prop_validate = 0.50\n",
    "    \n",
    "if prop_validate == 0:\n",
    "    print('You need to validate.')\n",
    "    prop_train = 0.50\n",
    "    prop_validate = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize loss, accuracy and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_class_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_adv_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# class_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# adv_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "class_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "adv_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "optimizer_class=tf.keras.optimizers.Adam(lr=HPARAMS.classifier_lr)\n",
    "optimizer_adv=tf.keras.optimizers.Adam(lr=HPARAMS.adversary_lr)\n",
    "optimizer=tf.keras.optimizers.Adam(lr=HPARAMS.classifier_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and combine models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_inputs = []\n",
    "xlayers = []\n",
    "\n",
    "n = len(layer_names)\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    if('theta' in layer_names[i]):\n",
    "        \n",
    "        inpt = tf.keras.Input(shape = (1,), name=layer_names[i])\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(inpt)\n",
    "        classifier_inputs.append(inpt)\n",
    "        xlayers.append(x)\n",
    "\n",
    "        x_dense = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        xlayers.append(x_dense)\n",
    "\n",
    "    else:\n",
    "\n",
    "        inpt = tf.keras.Input(shape = (10,1), name=layer_names[i])\n",
    "        x = tf.keras.layers.Flatten()(inpt)\n",
    "\n",
    "        classifier_inputs.append(inpt)\n",
    "        xlayers.append(x) \n",
    "\n",
    "if(n > 1):\n",
    "    concatenated_layers_classifier = tf.keras.layers.concatenate(inputs=xlayers, axis=-1)\n",
    "\n",
    "if(n==1):\n",
    "    classifier_inputs = classifier_inputs[0]\n",
    "\n",
    "classifier_inputs_dict = {c.name:c for c in classifier_inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for adversary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_from_model = tf.keras.layers.Input(shape = (2,), name='Input')\n",
    "x_input = tf.keras.layers.Flatten()(input_from_model)\n",
    "\n",
    "feature_input = tf.keras.layers.Input(shape = (1,), name='theta_2')\n",
    "x_feature = tf.keras.layers.Flatten()(feature_input)\n",
    "\n",
    "concatenated_layers_adv = tf.keras.layers.concatenate(inputs=[x_feature,x_input], axis=-1)\n",
    "adversary_inputs = [input_from_model, x_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier and Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = build_classifier(classifier_inputs, concatenated_layers_classifier)\n",
    "adversary = build_adversary(adversary_inputs, concatenated_layers_adv)\n",
    "\n",
    "classifier_output = classifier(classifier_inputs)\n",
    "adversary_output = adversary(adversary_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {**classifier_inputs_dict,\n",
    "            'Input':input_from_model,\n",
    "            'theta_2':feature_input,\n",
    "    }\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(\n",
    "    inputs=model_inputs,\n",
    "    outputs={'c_out':classifier_output,\n",
    "             'a_out':adversary_output,\n",
    "            },\n",
    "    \n",
    "    name='Classifier_and_Adversary',\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train adversary for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier.compile(\n",
    "    loss=loss_class_fn,\n",
    "    optimizer=optimizer_class,\n",
    ")\n",
    "\n",
    "adversary.compile(\n",
    "    loss=loss_adv_fn,\n",
    "    optimizer=optimizer_adv,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifierEpochs = 1\n",
    "adversaryEpochs=10\n",
    "\n",
    "if(classifierOnly):\n",
    "    classifierEpochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, Y_train, epochs=classifierEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(not classifierOnly):\n",
    "    adversary.fit([classifier(X_train), X_train['theta']], theta_labels, epochs=adversaryEpochs, batch_size=HPARAMS.batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to not get thousands of warnings about gradients\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLambda = False\n",
    "\n",
    "if(trainLambda):\n",
    "\n",
    "    trainable_lambda = tf.Variable(2., dtype='float32', name='lambda',\n",
    "                                   constraint=lambda x: tf.clip_by_value(x, 1, np.infty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def calc_loss_total(loss_c, loss_a, Lambda):\n",
    "\n",
    "    # loss_c  - loss of classifier\n",
    "    # loss_a  - loss of adversary\n",
    "    \n",
    "    loss_c = tf.cast(loss_c, 'float32')\n",
    "    loss_a = tf.cast(loss_a, 'float32')\n",
    "\n",
    "    L = tf.reduce_mean(tf.math.abs(tf.reduce_mean(loss_c - Lambda * loss_a)))\n",
    "    \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(X_c, Y_c, X_a, Y_a, classifier, adversary, model,\n",
    "               acc, Lfn_a, Lfn_c, optimizer, Lambda, trainLambda=False):\n",
    "    \n",
    "    # X_c : classifier dataset for batch\n",
    "    # Y_c : classifier labels for batch\n",
    "    # X_a : adversary dataset for batch\n",
    "    # Y_a : adversary labels for batch\n",
    "    # Lfn_c : classifier loss function\n",
    "    # Lfn_a : adversary loss function\n",
    "    # acc : accuracy metric\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape_class:\n",
    "        \n",
    "        # adversary output and labels for the batch\n",
    "        logits_adv = adversary([classifier(X_c), X_a])\n",
    "    \n",
    "        # classifier output for the batch\n",
    "        logits_class = classifier(X_c) \n",
    "\n",
    "        # get classifier loss\n",
    "#         loss_class = Lfn_c(logits_class, Y_c)\n",
    "        loss_class = Lfn_c(Y_c, logits_class)\n",
    "\n",
    "        # get adversary loss\n",
    "        loss_adv = Lfn_a(Y_a, logits_adv)\n",
    "        \n",
    "        tape_class.watch(loss_class)\n",
    "        tape_class.watch(loss_adv)\n",
    "        \n",
    "        # get combined loss\n",
    "        loss_total = calc_loss_total(loss_class, loss_adv, Lambda)\n",
    "        \n",
    "        tape_class.watch(loss_total)\n",
    "        tape_class.watch(model.trainable_weights)\n",
    "\n",
    "        # get and apply gradients\n",
    "        grads = tape_class.gradient(loss_total, model.trainable_weights)\n",
    "        if(trainLambda): grads_l = tape_class.gradient(loss_total, [Lambda])\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        if(trainLambda): optimizer.apply_gradients(zip(grads_l, [Lambda]))\n",
    "\n",
    "        acc.update_state(tf.reshape(Y_c, (*Y_c.shape, 1)), tf.reshape(logits_class, (*logits_class.shape, 1)))\n",
    "\n",
    "        tape_class.reset()\n",
    "    \n",
    "    return loss_total, loss_class, loss_adv, acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['loss_class'] = list([])\n",
    "metrics['loss_adv'] = list([])\n",
    "metrics['loss_total'] = list([])\n",
    "metrics['acc_class'] = list([])\n",
    "metrics['acc_adv'] = list([])\n",
    "metrics['lambda'] = list([])\n",
    "metrics['epoch'] = list([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatchSteps = nEvents*HPARAMS.valSplit / HPARAMS.batchsize\n",
    "loss_batch = 0\n",
    "loss_batch_a = 0\n",
    "loss_batch_c = 0\n",
    "acc_batch = 0\n",
    "total_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if (not classifierOnly):\n",
    "\n",
    "    last_epoch=time.time()\n",
    "    start_training = time.time()\n",
    " \n",
    "    nEpochs = 1\n",
    "\n",
    "    nUpdates_Classifier = HPARAMS.nUpdates_Classifier\n",
    "#     nUpdates_Classifier = 20\n",
    "\n",
    "    for ii in range(nEpochs):\n",
    "\n",
    "        start_update=time.time()\n",
    "\n",
    "        for epoch in range(nUpdates_Classifier):\n",
    "            \n",
    "            loss_batch = 0\n",
    "            loss_batch_c = 0\n",
    "            loss_batch_a = 0\n",
    "            acc_batch = 0\n",
    "\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                \n",
    "                loss_total, loss_class, loss_adv, acc_class = train_step(x_batch_train,\n",
    "                                                                         y_batch_train, \n",
    "                                                                         train_dataset_theta[step],\n",
    "                                                                         train_dataset_theta_Y[step],\n",
    "                                                                         classifier, adversary, model,\n",
    "                                                                         class_acc_metric,\n",
    "                                                                         loss_class_fn, \n",
    "                                                                         loss_adv_fn,\n",
    "                                                              optimizer, HPARAMS.Lambda, trainLambda)\n",
    "                \n",
    "                \n",
    "            \n",
    "                    \n",
    "                \n",
    "                \n",
    "                loss_batch += loss_total.numpy()\n",
    "                loss_batch_c += loss_class.numpy()\n",
    "                loss_batch_a += loss_adv.numpy()\n",
    "                acc_batch += acc_class.numpy()\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "            for step, (x_batch, y_batch) in enumerate(valid_dataset):\n",
    "\n",
    "\n",
    "                val_logits = classifier(x_batch, training = False)\n",
    "                \n",
    "                class_acc_metric.update_state(tf.reshape(y_batch, (*y_batch.shape,1)),\n",
    "                                              tf.reshape(val_logits, (*val_logits.shape,1)))\n",
    "\n",
    "            \n",
    "    \n",
    "            metrics['acc_class'].append(class_acc_metric.result().numpy())\n",
    "            class_acc_metric.reset_states()\n",
    "\n",
    "            loss_epoch   = loss_batch   / nBatchSteps\n",
    "            loss_epoch_c = loss_batch_c / nBatchSteps\n",
    "            loss_epoch_a = loss_batch_a / nBatchSteps\n",
    "            acc_epoch    = acc_batch    / nBatchSteps\n",
    "            \n",
    "\n",
    "                \n",
    "            print('Epoch {0:0.0f}   Loss: {1:0.4f}  Loss class: {2:0.4f}  Loss adv: {3:0.4f}  val acc: {4:0.4f}  {5}'.format(\n",
    "                total_epochs,\n",
    "                loss_epoch,\n",
    "                loss_epoch_c,\n",
    "                loss_epoch_a,\n",
    "                acc_epoch,\n",
    "                printTime(time.time() - last_epoch, returnString=True)\n",
    "\n",
    "            ))\n",
    "            \n",
    "            metrics['loss_total'].append(loss_epoch)\n",
    "            metrics['loss_class'].append(loss_epoch_c)\n",
    "            metrics['loss_adv'].append(loss_epoch_a)\n",
    "            metrics['acc_class'].append(acc_epoch)\n",
    "            metrics['epoch'].append(total_epochs)\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "\n",
    "            if(trainLambda): print('lambda {0:0.4f}'.format(trainable_lambda.numpy()))\n",
    "\n",
    "            last_epoch = time.time()\n",
    "\n",
    "            if(trainLambda): metrics['lambda'].append(trainable_lambda.numpy())\n",
    "\n",
    "            print('train adversary ', end='')\n",
    "            adversary.fit([classifier(X_train), X_train['theta']], theta_labels, epochs=1, batch_size=HPARAMS.batchsize)\n",
    "\n",
    "            \n",
    "            total_epochs += 1\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        end_update = time.time()\n",
    "        printTime(end_update - start_update)\n",
    "\n",
    "            \n",
    "\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    end_training=time.time()\n",
    "    print('\\nTotal time: ', end='')\n",
    "    printTime(end_training - start_training)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model names \n",
    "\n",
    "model_string = ''\n",
    "for name in layer_names:\n",
    "    model_string += '_' + name\n",
    "if(classifierOnly):\n",
    "    model_string += '_classifierOnly'\n",
    "model_name = 'model' + model_string\n",
    "model_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models \n",
    "\n",
    "tf.keras.models.save_model(model, f'models/{model_string}.h5')\n",
    "print('saving '+'models/'+model_name+'.h5')\n",
    "if (not classifierOnly):\n",
    "    tf.keras.models.save_model(classifier, f'models/{model_name}_classifier.h5')\n",
    "    tf.keras.models.save_model(adversary, f'models/{model_name}_adversary.h5')\n",
    "\n",
    "    print(f'saving models/{model_name}_classifier.h5')\n",
    "    print(f'saving models/{model_name}_adversary.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap = plt.get_cmap('Accent')\n",
    "colors = np.linspace(0,1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "\n",
    "lw=3\n",
    "\n",
    "\n",
    "plt.plot(metrics['epoch'], metrics['loss_total'], lw=lw, color=cmap(colors[0]), label=r'$\\mathcal{L}$')\n",
    "plt.plot(metrics['epoch'], metrics['loss_class'], lw=lw, color=cmap(colors[1]), label=r'$\\mathcal{L}_\\mathcal{class}$')\n",
    "plt.plot(metrics['epoch'], HPARAMS.Lambda*np.array(metrics['loss_adv']), lw=lw, color=cmap(colors[2]), label=r'$\\mathcal{\\lambda} \\mathcal{L}_\\mathcal{adv}$')\n",
    "\n",
    "# plt.plot(x1, HPARAMS.Lambda*np.average(loss_avg_a, axis=0), lw=lw, color=cmap(colors[2]), label=r'$\\lambda \\mathcal{L_{adv}}$')\n",
    "# plt.plot(x1, np.average(loss_avg_c, axis=0) - 100*np.average(loss_avg_a, axis=0), lw=lw, color=cmap(colors[3]), label='Loss: Total')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylim(0,2)\n",
    "plt.xticks(metrics['epoch'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('Steps', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10,7.5))\n",
    "# x_epochs = np.arange(len(metrics['loss_total']), dtype=int) + 1\n",
    "\n",
    "# lw=3\n",
    "\n",
    "\n",
    "# plt.plot(x_epochs, metrics['loss_total'], lw=lw, color=cmap(colors[0]), label='Loss: Total')\n",
    "# # plt.plot(x_epochs, metrics['loss_class'], lw=lw, color=cmap(colors[1]), label='Loss: Classifier')\n",
    "# # plt.plot(x_epochs, HPARAMS.Lambda*np.array(metrics['loss_adv']), lw=lw, color=cmap(colors[2]), label='Loss: Adversary')\n",
    "# plt.legend(fontsize=15)\n",
    "# # plt.ylim(-0.1,10.1)\n",
    "# plt.xticks(x_epochs[::20], fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.xlabel('Steps', fontsize=15)\n",
    "# plt.ylabel('Loss', fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# fig = plt.figure(figsize=(7,5))\n",
    "# x_epochs = np.arange(len(metrics['acc_class']), dtype=int) + 1\n",
    "\n",
    "# lw=3\n",
    "\n",
    "\n",
    "# plt.plot(x_epochs, metrics['acc_class'], lw=lw, color=cmap(colors[4]), label='Val Accuracy')\n",
    "# # plt.plot(x_epochs, metrics['loss_class'], lw=lw, color=cmap(colors[1]), label='Loss: Classifier')\n",
    "# # plt.plot(x_epochs, HPARAMS.Lambda*np.array(metrics['loss_adv']), lw=lw, color=cmap(colors[2]), label='Loss: Adversary')\n",
    "# plt.legend(fontsize=15)\n",
    "# # plt.ylim(-0.1,10.1)\n",
    "# plt.xticks(x_epochs[::40], fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.xlabel('Steps', fontsize=15)\n",
    "# plt.ylabel('Loss', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable dictionaries for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = {name:X_train[name].squeeze() for name in layer_names}\n",
    "\n",
    "for key in xvars.keys():\n",
    "    \n",
    "    if(len(xvars[key].shape) > 1):\n",
    "        xvars[key] = np.average(xvars[key], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = classifier(X_train)\n",
    "predict_adv = adversary([predict, X_train['theta']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment to load saved models instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model('models/model_theta_tau1_tau2.h5')\n",
    "# loaded_classifier = tf.keras.models.load_model('models/model_theta_tau1_tau2_classifier.h5')\n",
    "# loaded_adversary = tf.keras.models.load_model('models/model_theta_tau1_tau2_adversary.h5')\n",
    "\n",
    "# predict = loaded_classifier(X_train)\n",
    "# predict_adv = loaded_adversary([predict, X_train['theta']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = predict[Y_train[:,1]==1][:,1].numpy()\n",
    "bkg = predict[Y_train[:,0]==1][:,1].numpy()\n",
    "\n",
    "bins = np.linspace(0, 1, 20)\n",
    "\n",
    "plt.hist(sig, bins, alpha=0.5, label='Signal', density=True)\n",
    "plt.hist(bkg, bins, alpha=0.5, label='Background', density=True)\n",
    "# plt.ylim(0,25000)\n",
    "# plt.yscale('log')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = Nlist // 2 + Nlist % 2\n",
    "cols = 2\n",
    "fig, ax = plt.subplots(rows,cols, figsize=(10,4*rows))\n",
    "\n",
    "Title = 'Model Inputs:'\n",
    "for name in layer_names:\n",
    "    Title += ', ' + xlabels[name]\n",
    "    \n",
    "Title = Title.replace(':,', ':')\n",
    "\n",
    "\n",
    "\n",
    "xvals_list = []\n",
    "yvals_list = []\n",
    "yerrs_list = []\n",
    "\n",
    "xvals_list_bkg = []\n",
    "yvals_list_bkg = []\n",
    "yerrs_list_bkg = []\n",
    "\n",
    "for name in layer_names:\n",
    "    \n",
    "    x, y, e = get_prediction_yvalues(xvars[name], predict[:,1])\n",
    "    xb, yb, eb = get_prediction_yvalues(xvars[name], predict[:,0])\n",
    "    \n",
    "    xvals_list.append(x)\n",
    "    yvals_list.append(y)\n",
    "    yerrs_list.append(e)\n",
    "\n",
    "    xvals_list_bkg.append(xb)\n",
    "    yvals_list_bkg.append(yb)\n",
    "    yerrs_list_bkg.append(eb)\n",
    "    \n",
    "    \n",
    "    \n",
    "if (Nlist > 0):\n",
    "    if(Nlist < 2):\n",
    "        ax1 = ax[0]\n",
    "    elif(Nlist < 3):\n",
    "        ax1 = ax[0]\n",
    "    else:\n",
    "        ax1 = ax[0][0]\n",
    "        \n",
    "        \n",
    "if (Nlist > 1):\n",
    "    if(Nlist < 3):\n",
    "        ax2 = ax[1]\n",
    "    else:\n",
    "        ax2 = ax[0][1]\n",
    "        \n",
    "\n",
    "if (Nlist > 2): ax3 = ax[1][0]\n",
    "if (Nlist > 3): ax4 = ax[1][1]\n",
    "if (Nlist > 4): ax5 = ax[2][0]\n",
    "if (Nlist > 5): ax6 = ax[2][1]\n",
    "    \n",
    "    \n",
    "fmt = ''\n",
    "lw = 3\n",
    "\n",
    "if (Nlist > 0):\n",
    "    \n",
    "    ax1.errorbar(xvals_list[0], yvals_list[0], yerr=yerrs_list[0], lw=lw, fmt=fmt)\n",
    "    ax1.errorbar(xvals_list_bkg[0], yvals_list_bkg[0], yerr=yerrs_list_bkg[0], lw=lw, fmt=fmt)\n",
    "    ax1.set_xlabel(xlabels[layer_names[0]])\n",
    "    ax1.set_ylabel(r'Output')    \n",
    "    \n",
    "if (Nlist > 1):\n",
    "    \n",
    "    ax2.errorbar(xvals_list[1], yvals_list[1], yerr=yerrs_list[1], lw=lw, fmt=fmt)\n",
    "    ax2.errorbar(xvals_list_bkg[1], yvals_list_bkg[1], yerr=yerrs_list_bkg[1], lw=lw, fmt=fmt)\n",
    "    ax2.set_xlabel(xlabels[layer_names[1]])\n",
    "    ax2.set_ylabel(r'Output')\n",
    "    \n",
    "if (Nlist > 2):\n",
    "    \n",
    "    ax3.errorbar(xvals_list[2], yvals_list[2], yerr=yerrs_list[2], lw=lw, fmt=fmt)\n",
    "    ax3.errorbar(xvals_list_bkg[2], yvals_list_bkg[2], yerr=yerrs_list_bkg[2], lw=lw, fmt=fmt)\n",
    "    ax3.set_xlabel(xlabels[layer_names[2]])\n",
    "    ax3.set_ylabel(r'Output')\n",
    "\n",
    "if (Nlist > 3):\n",
    "    \n",
    "    ax4.errorbar(xvals_list[3], yvals_list[3], yerr=yerrs_list[3], lw=lw, fmt=fmt)\n",
    "    ax4.errorbar(xvals_list_bkg[3], yvals_list_bkg[3], yerr=yerrs_list_bkg[3], lw=lw, fmt=fmt)\n",
    "    ax4.set_xlabel(xlabels[layer_names[3]])\n",
    "    ax4.set_ylabel(r'Output')\n",
    "    \n",
    "if (Nlist > 4):\n",
    "    \n",
    "    ax5.errorbar(xvals_list[4], yvals_list[4], yerr=yerrs_list[4], lw=lw, fmt=fmt)\n",
    "    ax5.errorbar(xvals_list_bkg[4], yvals_list_bkg[4], yerr=yerrs_list_bkg[4], lw=lw, fmt=fmt)\n",
    "    ax5.set_xlabel(xlabels[layer_names[4]])\n",
    "    ax5.set_ylabel(r'Output')\n",
    "    \n",
    "if (Nlist > 5):\n",
    "    \n",
    "    ax6.errorbar(xvals_list[5], yvals_list[5], yerr=yerrs_list[5], lw=lw, fmt=fmt)\n",
    "    ax6.errorbar(xvals_list_bkg[5], yvals_list_bkg[5], yerr=yerrs_list_bkg[5], lw=lw, fmt=fmt)\n",
    "    ax6.set_xlabel(xlabels[layer_names[5]])\n",
    "    ax6.set_ylabel(r'Output')   \n",
    "\n",
    "fig.legend(['Signal', 'Background'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "figname = 'plots/Lambda'+str(HPARAMS.Lambda)+'_scatterplots.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "\n",
    "plt.suptitle(Title, fontsize=25)\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(rows,cols, figsize=(10,4*rows))\n",
    "\n",
    "plt.suptitle(Title, fontsize=25)\n",
    "\n",
    "xvals_list = []\n",
    "yvals_list = []\n",
    "yerrs_list = []\n",
    "\n",
    "xvals_list_bkg = []\n",
    "yvals_list_bkg = []\n",
    "yerrs_list_bkg = []\n",
    "\n",
    "\n",
    "yrange = [-0.05,1.05]\n",
    "\n",
    "for name in layer_names:\n",
    "    \n",
    "    x, y, e = get_prediction_yvalues(xvars[name], predict[:,1], norm=True)\n",
    "    xb, yb, eb = get_prediction_yvalues(xvars[name], predict[:,0], norm=True)\n",
    "    \n",
    "    xvals_list.append(x)\n",
    "    yvals_list.append(y)\n",
    "    yerrs_list.append(e)\n",
    "\n",
    "    xvals_list_bkg.append(xb)\n",
    "    yvals_list_bkg.append(yb)\n",
    "    yerrs_list_bkg.append(eb)\n",
    "    \n",
    "    \n",
    "    \n",
    "if (Nlist > 0):\n",
    "    if(Nlist < 2):\n",
    "        ax1 = ax[0]\n",
    "    elif(Nlist < 3):\n",
    "        ax1 = ax[0]\n",
    "    else:\n",
    "        ax1 = ax[0][0]\n",
    "        \n",
    "        \n",
    "if (Nlist > 1):\n",
    "    if(Nlist < 3):\n",
    "        ax2 = ax[1]\n",
    "    else:\n",
    "        ax2 = ax[0][1]\n",
    "        \n",
    "\n",
    "if (Nlist > 2): ax3 = ax[1][0]\n",
    "if (Nlist > 3): ax4 = ax[1][1]\n",
    "if (Nlist > 4): ax5 = ax[2][0]\n",
    "if (Nlist > 5): ax6 = ax[2][1]\n",
    "\n",
    "if (Nlist > 0):\n",
    "    \n",
    "    ax1.errorbar(xvals_list[0], yvals_list[0], yerr=yerrs_list[0], lw=lw, fmt=fmt)\n",
    "    ax1.errorbar(xvals_list_bkg[0], yvals_list_bkg[0], yerr=yerrs_list_bkg[0], lw=lw, fmt=fmt)\n",
    "    ax1.set_xlabel(xlabels[layer_names[0]])\n",
    "    ax1.set_ylabel(r'Normalized Output')\n",
    "    ax1.set_ylim(yrange)\n",
    "    \n",
    "if (Nlist > 1):\n",
    "    \n",
    "    ax2.errorbar(xvals_list[1], yvals_list[1], yerr=yerrs_list[1], lw=lw, fmt=fmt)\n",
    "    ax2.errorbar(xvals_list_bkg[1], yvals_list_bkg[1], yerr=yerrs_list_bkg[1], lw=lw, fmt=fmt)\n",
    "    ax2.set_xlabel(xlabels[layer_names[1]])\n",
    "    ax2.set_ylabel(r'Normalized Output')\n",
    "    ax2.set_ylim(yrange)\n",
    "    \n",
    "if (Nlist > 2):\n",
    "    \n",
    "    ax3.errorbar(xvals_list[2], yvals_list[2], yerr=yerrs_list[2], lw=lw, fmt=fmt)\n",
    "    ax3.errorbar(xvals_list_bkg[2], yvals_list_bkg[2], yerr=yerrs_list_bkg[2], lw=lw, fmt=fmt)\n",
    "    ax3.set_xlabel(xlabels[layer_names[2]])\n",
    "    ax3.set_ylabel(r'Normalized Output')\n",
    "    ax3.set_ylim(yrange)\n",
    "\n",
    "if (Nlist > 3):\n",
    "    \n",
    "    ax4.errorbar(xvals_list[3], yvals_list[3], yerr=yerrs_list[3], lw=lw, fmt=fmt)\n",
    "    ax4.errorbar(xvals_list_bkg[3], yvals_list_bkg[3], yerr=yerrs_list_bkg[3], lw=lw, fmt=fmt)\n",
    "    ax4.set_xlabel(xlabels[layer_names[3]])\n",
    "    ax4.set_ylabel(r'Normalized Output')\n",
    "    ax4.set_ylim(yrange)\n",
    "    \n",
    "if (Nlist > 4):\n",
    "    \n",
    "    ax5.errorbar(xvals_list[4], yvals_list[4], yerr=yerrs_list[4], lw=lw, fmt=fmt)\n",
    "    ax5.errorbar(xvals_list_bkg[4], yvals_list_bkg[4], yerr=yerrs_list_bkg[4], lw=lw, fmt=fmt)\n",
    "    ax5.set_xlabel(xlabels[layer_names[4]])\n",
    "    ax5.set_ylabel(r'Normalized Output')\n",
    "    ax5.set_ylim(yrange)\n",
    "    \n",
    "if (Nlist > 5):\n",
    "    \n",
    "    ax6.errorbar(xvals_list[5], yvals_list[5], yerr=yerrs_list[5], lw=lw, fmt=fmt)\n",
    "    ax6.errorbar(xvals_list_bkg[5], yvals_list_bkg[5], yerr=yerrs_list_bkg[5], lw=lw, fmt=fmt)\n",
    "    ax6.set_xlabel(xlabels[layer_names[5]])\n",
    "    ax6.set_ylabel(r'Normalized Output')\n",
    "    ax6.set_ylim(yrange)\n",
    "\n",
    "fig.legend(['Signal', 'Background'])\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "figname = 'plots/Lambda'+str(HPARAMS.Lambda)+'_scatterplots_norm.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Adversary Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows=2\n",
    "cols=3\n",
    "\n",
    "fig, ax = plt.subplots(rows,cols, figsize=(15,7))\n",
    "\n",
    "if(HPARAMS.thetaBins > 0): plotBin(1, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "if(HPARAMS.thetaBins > 1): plotBin(2, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "if(HPARAMS.thetaBins > 2): plotBin(3, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "if(HPARAMS.thetaBins > 3): plotBin(4, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "if(HPARAMS.thetaBins > 4): plotBin(5, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "if(HPARAMS.thetaBins > 5): plotBin(6, xvars['theta'], predict_adv, rows, cols, ax)\n",
    "    \n",
    "plt.tight_layout()\n",
    "figname = 'plots/Adversary_Output.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTime(time.time()-start_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
