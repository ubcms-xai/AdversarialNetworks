{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "\n",
    "print('Tensorflow Version {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_notebook=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self):\n",
    "        self.Lambda = 1\n",
    "        self.batchsize = 50\n",
    "        self.trainingBatchSize = 5\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTime(delta):\n",
    "    \n",
    "    h = (delta // 3600)\n",
    "    m = (delta % 3600) // 60\n",
    "    s = delta % 60\n",
    "    \n",
    "    if (delta > 3600):\n",
    "        print('Time {0:.0f}h {1:.0f}m {2:.2f}s'.format(h, m, s))\n",
    "        \n",
    "    elif (delta > 60):\n",
    "        print('Time {0:.0f}m {1:.2f}s'.format(m, s))\n",
    "    \n",
    "    else:\n",
    "        print('Time {0:.2f}s'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "def plotFeature(X, Y, feat):\n",
    "    \n",
    "    siglab = Y[:,1]==1\n",
    "    bkglab = Y[:,1]==0\n",
    "    \n",
    "    \n",
    "    \n",
    "    sig = X[feat][siglab].flatten()\n",
    "    bkg = X[feat][bkglab].flatten()\n",
    "    \n",
    "    maximum = np.max([np.max(sig), np.max(bkg)])\n",
    "    minimum = np.min([np.min(sig), np.min(bkg)])\n",
    "    nbins = 50\n",
    "    brange = np.linspace(minimum, maximum, 50)\n",
    "    \n",
    "    \n",
    "    plt.hist(sig, bins=brange, hatch='//', alpha=alpha, label='Signal')\n",
    "    plt.hist(sig, bins=brange, histtype='step', color='k')\n",
    "    plt.hist(bkg, bins=brange, hatch='\\\\', alpha=alpha, label='Background')\n",
    "    plt.hist(bkg, bins=brange, histtype='step', color='k')\n",
    "    \n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Counts')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdversary(X, X_shift, Y):\n",
    "    \n",
    "    sig = (Y[:,1]==1)\n",
    "    \n",
    "    xval = X[sig].flatten()\n",
    "    xval_s = X_shift[sig].flatten()\n",
    "\n",
    "    maximum = np.max([np.max(X), np.max(X_shift)])\n",
    "    minimum = np.min([np.min(X), np.min(X_shift)])\n",
    "    nbins = 50\n",
    "    brange = np.linspace(minimum, maximum, 50)\n",
    "\n",
    "    hx = plt.hist(xval, bins=brange, histtype='step', color='k', label='Signal')\n",
    "    hxs = plt.hist(xval_s, bins=brange, histtype='step', color='darkgray', label='Perturbed Signal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    hx_height = hx[0]\n",
    "    hxs_height = hxs[0]\n",
    "    height = hx_height-hxs_height\n",
    "    xaxis = hx[1][1:]\n",
    "   \n",
    "    plt.plot(xaxis, height, color='k', linewidth=2, label='Adversary')\n",
    "    plt.legend()\n",
    "    \n",
    "    label_adv = np.ones_like(height)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return height, label_adv\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets as dictionaries\n",
    "### Datasets generated with ToyModel/makeFourVectors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by running makeFourVectors.ipynb with addPerturbation = False\n",
    "data_train = np.load('data/jetConstTrain_overlap.npz')\n",
    "data_test = np.load('data/jetConstTest_overlap.npz')\n",
    "\n",
    "# created by running makeFourVectors.ipynb with addPerturbation = True\n",
    "data_train_shift = np.load('data/jetConstTrain_overlap_perturb.npz')\n",
    "data_test_shift = np.load('data/jetConstTest_overlap_perturb.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_all = ['pt', 'eta', 'phi', 'mass', 'thetalab', 'radiilab', 'zlab']\n",
    "\n",
    "\n",
    "feat_list = ['thetalab', 'pt', 'eta', 'phi']\n",
    "feat_xaug = []\n",
    "\n",
    "Nlist = len(feat_list)\n",
    "Nxaug = len(feat_xaug)\n",
    "\n",
    "nEvents = len(data_train['pt'].flatten())\n",
    "\n",
    "print('Number of particle list features: ', Nlist)\n",
    "print('Number of XAUG features: ', Nxaug)\n",
    "print('N Events: ', nEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets as dictionaries\n",
    "\n",
    "X_train = {key: data_train[key] for key in feat_list}\n",
    "X_test = {key: data_test[key] for key in feat_list}\n",
    "\n",
    "X_train_shift = {key: data_train_shift[key] for key in feat_list}\n",
    "X_test_shift = {key: data_test_shift[key] for key in feat_list}\n",
    "\n",
    "\n",
    "\n",
    "# get labels\n",
    "\n",
    "Y_train = data_train['labels']\n",
    "Y_test = data_test['labels']\n",
    "\n",
    "Y_train_shift = data_train_shift['labels']\n",
    "Y_test_shift = data_test_shift['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, var in data_test.items():\n",
    "    print(var.shape, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [key for key in X_train.keys() if not 'labels' in key]\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make $\\mathrm{log}(p_{T})$ and $\\mathrm{log}(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['pt'] = np.log(X_train['pt'])\n",
    "# X_train['zlab'] = np.log(X_train['zlab'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'thetalab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'eta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal difference adversary\n",
    "Note: This is not currently used as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train, Y_train, 'thetalab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeature(X_train_shift, Y_train_shift, 'thetalab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adv, Y_adv = getAdversary(X_train['thetalab'], X_train_shift['thetalab'], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin $\\Delta \\theta$\n",
    "`binned_theta` is an input to the adversary model and\n",
    "`theta_labels` are the labels for the adversary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape theta\n",
    "X_train['thetalab'] = X_train['thetalab'][:,0]\n",
    "\n",
    "thetabins = np.linspace(0, 0.3, 11)\n",
    "binned_theta = np.digitize(X_train['thetalab'], thetabins) + 1\n",
    "theta_labels = tf.one_hot(binned_theta.squeeze(), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nfirst 5 bins\\n')\n",
    "print(binned_theta[:5])\n",
    "print('\\nfirst 5 labels\\n')\n",
    "print(theta_labels.numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train['thetalab'].flatten(), bins=thetabins, histtype='step', density=True)\n",
    "plt.xlabel(r'$\\Delta \\theta$')\n",
    "plt.ylabel('Density')\n",
    "plt.title(r'Binned $\\Delta \\theta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(n, nex):\n",
    "    inpts = []\n",
    "    xlayers = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        if('theta' in layer_names[i]):\n",
    "\n",
    "\n",
    "            inpt = tf.keras.Input(shape = (1,), name=layer_names[i])\n",
    "\n",
    "            x = tf.keras.layers.Flatten()(inpt)\n",
    "            inpts.append(inpt)\n",
    "            xlayers.append(x)\n",
    "\n",
    "            x_dense = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "            xlayers.append(x_dense)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            inpt = tf.keras.Input(shape = (10,1), name=layer_names[i])\n",
    "            x = tf.keras.layers.Flatten()(inpt)\n",
    "\n",
    "            inpts.append(inpt)\n",
    "            xlayers.append(x) \n",
    "    \n",
    "    if(n > 1):\n",
    "        x = tf.keras.layers.concatenate(inputs=xlayers, axis=-1)\n",
    "        \n",
    "    if(n==1):\n",
    "        Inputs = inpts[0]\n",
    "    else:\n",
    "        Inputs = inpts\n",
    "        \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    output = tf.keras.layers.Dense(2, activation='softmax', name = 'labels')(x) \n",
    "    model = tf.keras.Model(inputs=Inputs, outputs=output, name='Classifier')\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = build_classifier(Nlist, Nxaug)\n",
    "model_class.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_classifier(n, nex):\n",
    "    \n",
    "#     inpts = []\n",
    "#     xaugs = []\n",
    "#     xlayers = []\n",
    "    \n",
    "\n",
    "    \n",
    "#     # loop over all input variables\n",
    "#     for i in range(n):\n",
    "        \n",
    "#         # particle list inputs \n",
    "#         if(i < n-nex):\n",
    "            \n",
    "#             if('theta' in layer_names[i]):\n",
    "#                 inpt = tf.keras.Input(shape = (1,), name=layer_names[i])\n",
    "                \n",
    "#                 x = tf.keras.layers.Flatten()(inpt)\n",
    "\n",
    "#                 inpts.append(inpt)\n",
    "#                 xlayers.append(x)\n",
    "                \n",
    "#                 x_dense = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "#                 xlayers.append(x_dense)\n",
    "                \n",
    "#             else:\n",
    "#                 inpt = tf.keras.Input(shape = (10,1), name=layer_names[i])\n",
    "\n",
    "\n",
    "\n",
    "# #             x = tf.keras.layers.Conv1D(64, 3, padding = 'same', activation='relu')(inpt)\n",
    "# #             x = tf.keras.layers.Conv1D(64, 1, padding = 'same', activation='relu')(x)\n",
    "# #             x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# #             x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "# #             x = tf.keras.layers.Conv1D(32, 3, padding = 'same', activation='relu')(x)\n",
    "# #             x = tf.keras.layers.Conv1D(32, 1, padding = 'same', activation='relu')(x)\n",
    "# #             x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# #             x = tf.keras.layers.MaxPool1D()(x)\n",
    "\n",
    "#                 x = tf.keras.layers.Flatten()(inpt)\n",
    "\n",
    "#                 inpts.append(inpt)\n",
    "#                 xlayers.append(x)\n",
    "        \n",
    "#         elif((nex > 0)):\n",
    "#             inpt = tf.keras.layers.Input(shape = (1,), name=layer_names[i])\n",
    "#             xaugs.append(inpt)\n",
    "    \n",
    "#     #concatenation of particle list inputs with expert variable inputs\n",
    "    \n",
    "#     if(n > 1):\n",
    "#         x = tf.keras.layers.concatenate(inputs=xlayers+xaugs, axis=-1)\n",
    "        \n",
    "        \n",
    "#     if(n==1):\n",
    "#         Inputs = inpts[0]\n",
    "#     else:\n",
    "#         Inputs = inpts+xaugs\n",
    "    \n",
    "    \n",
    "#     x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "   \n",
    "    \n",
    "#     output = tf.keras.layers.Dense(2, activation='softmax', name = 'labels')(x) \n",
    "#     model = tf.keras.Model(inputs=Inputs, outputs=output, name='Classifier')\n",
    "    \n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_class = build_classifier(Nlist, Nxaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_class.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adversary():\n",
    "    \n",
    "#     inputs = []\n",
    "    \n",
    "    input_from_model = tf.keras.layers.Input(shape = (2,), name='Input')\n",
    "    x_input = tf.keras.layers.Flatten()(input_from_model)\n",
    "        \n",
    "    feature_input = tf.keras.layers.Input(shape = (1,), name='theta')\n",
    "    x_feature = tf.keras.layers.Flatten()(feature_input)\n",
    "    \n",
    "#     x = tf.keras.layers.concatenate(inputs=[x_feature,x_input], axis=-1)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='Adversary_Dense64_2')(x_feature)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='Adversary_Dense128')(x)\n",
    "\n",
    "\n",
    "    \n",
    "    inputs = [input_from_model, feature_input]\n",
    "    output = tf.keras.layers.Dense(11, activation='softmax', name = 'output')(x) \n",
    "        \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='Adversary')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_adversary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier and Adversary Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "train_dataset = train_dataset.batch(HPARAMS.batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_adv = tf.data.Dataset.from_tensor_slices((X_train['thetalab'], theta_labels))\n",
    "train_dataset_adv = train_dataset_adv.batch(HPARAMS.batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_class_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_adv_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "optimizer_class=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "optimizer_adv=tf.keras.optimizers.Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = build_classifier(Nlist, Nxaug)\n",
    "model_adv = build_adversary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Gradient\n",
    "\n",
    "### Combined loss of classifier and adversary\n",
    "\n",
    "$$ \\mathcal{L}_{total} = \\mathcal{L}_{classifier} - \\lambda \\mathcal{L}_{adversary} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default custom gradient from tensorflow\n",
    "# change this\n",
    "\n",
    "print('lambda = {}'.format(HPARAMS.Lambda))\n",
    "\n",
    "@tf.custom_gradient\n",
    "def calc_grad_total(x, y, x_a, y_a):\n",
    "    \n",
    "    # x   - predictions of classifier\n",
    "    # y   - labels of classifier\n",
    "    # x_a - predicitions of adversary\n",
    "    # y_a - labels of adversary\n",
    "    \n",
    "    def grad(upstream):\n",
    "        \n",
    "        x0 = tf.where(tf.equal(x, 0), 1e-14, x)\n",
    "        x0_a = tf.where(tf.equal(x_a, 0), 1e-14, x_a)\n",
    "\n",
    "        one_minus_x = tf.where(tf.equal(x, 1), 1e-14, 1-x)\n",
    "        one_minus_x_a = tf.where(tf.equal(x_a, 1), 1e-14, 1-x_a)\n",
    "        \n",
    "     \n",
    "        dz_dy_c = -np.log(x0) + np.log(one_minus_x)\n",
    "\n",
    "        dz_dx_c = (-y / x0) + (1 - y) / (one_minus_x)\n",
    "\n",
    "        dz_dy_a = -np.log(x0_a) + np.log(one_minus_x_a) * (-HPARAMS.Lambda)\n",
    " \n",
    "        dz_dx_a = (-y_a / x0_a) + (1 - y_a) / (one_minus_x_a) * (-HPARAMS.Lambda)\n",
    "        \n",
    "        \n",
    "        return upstream * dz_dx_c, upstream * dz_dy_c, upstream * dz_dx_a, upstream * dz_dy_a\n",
    "    \n",
    "  \n",
    "        \n",
    "        \n",
    "    # categorical cross entropy\n",
    "    \n",
    "    \n",
    "\n",
    "    x0 = tf.where(tf.equal(x, 0), 1e-14, x)\n",
    "    x0_a = tf.where(tf.equal(x_a, 0), 1e-14, x_a)\n",
    "    \n",
    "    x1 = tf.where(tf.equal(x, 1), 1e-14, 1-x)\n",
    "    x1_a = tf.where(tf.equal(x_a, 1), 1e-14, 1-x_a)\n",
    "    \n",
    "    \n",
    "    y = tf.cast(y, 'float32')\n",
    "    \n",
    "    \n",
    "    z_c = -y * np.log(x0) - (1-y)*np.log(x1)   \n",
    "\n",
    "    z_a = -y_a * np.log(x0_a) - (1-y_a)*np.log(x1_a)\n",
    "\n",
    "\n",
    "    z = tf.math.reduce_mean(z_c) - HPARAMS.Lambda * tf.math.reduce_mean(z_a)\n",
    "    \n",
    "    return z, grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier and adversary separately for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class  = build_classifier(Nlist, Nxaug)\n",
    "model_adv = build_adversary()\n",
    "\n",
    "\n",
    "logits_adv = np.zeros((HPARAMS.batchsize, 11))\n",
    "y_batch_train_adv = np.zeros((HPARAMS.batchsize, 11))\n",
    "classifier_output = np.zeros((HPARAMS.batchsize, 2))\n",
    "\n",
    "begin = time.time()\n",
    "last_epoch=time.time()\n",
    "# train classifier first \n",
    "for epoch in range(1):\n",
    "    \n",
    "    print(\"\\nClassifier update %d\" % (1+epoch,))\n",
    "    printTime(time.time() - last_epoch)\n",
    "    last_epoch=time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.      \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        ### TRAIN CLASSIFIER ###\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "               \n",
    "            # logits for the batch\n",
    "            logits_class = model_class(x_batch_train)\n",
    "\n",
    "            # calculate loss\n",
    "            loss_class = loss_class_fn(y_batch_train, logits_class)\n",
    "            \n",
    "            # get and apply gradients\n",
    "            grads_class = tape.gradient(loss_class, model_class.trainable_weights)\n",
    "            \n",
    "            \n",
    "            optimizer_class.apply_gradients(zip(grads_class, model_class.trainable_weights))\n",
    "            \n",
    "    classifier_output = model_class(x_batch_train)\n",
    "            \n",
    "\n",
    "    \n",
    "# train adversary first\n",
    "\n",
    "last_epoch=time.time()\n",
    "for epoch in range(3):\n",
    "    \n",
    "    print(\"\\nAdversary update %d\" % (1+epoch,))\n",
    "    printTime(time.time() - last_epoch)\n",
    "    last_epoch=time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.      \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset_adv):\n",
    "\n",
    "        ### TRAIN ADVERSARY ###\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "               \n",
    "            # logits for the batch\n",
    "            logits_adv = model_adv([classifier_output, x_batch_train])\n",
    "\n",
    "            # calculate loss\n",
    "            loss_adv = loss_adv_fn(y_batch_train, logits_adv)\n",
    "            \n",
    "            \n",
    "            adversary_weights = model_adv.trainable_weights\n",
    "            \n",
    "            \n",
    "            # set adversary weights to 0\n",
    "            \n",
    "            #adversary_weights[0] = tf.Variable(adversary_weights[0].numpy()*[np.ones(64), np.zeros(64)], \n",
    "            #            name=adversary_weights[0].name, shape=adversary_weights[0].shape)\n",
    "            \n",
    "            \n",
    "            # get and apply gradients\n",
    "            grads_adv = tape.gradient(loss_adv, model_adv.trainable_weights)\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer_adv.apply_gradients(zip(grads_adv, model_adv.trainable_weights))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "print()            \n",
    "print('L_class', loss_class.numpy())\n",
    "print('L_adv', loss_adv.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "### Update each batch instead of each epoch\n",
    "\n",
    "1. Train classifier for 10 batches (HPARAMS.trainingBatchSize) with $ \\mathcal{L}_{total} = \\mathcal{L}_{classifier} - \\lambda \\mathcal{L}_{adversary} $\n",
    "2. Train adversary for 10 batches (HPARAMS.trainingBatchSize)\n",
    "3. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatches = nEvents // HPARAMS.batchsize\n",
    "nBatchLoops = nBatches // HPARAMS.trainingBatchSize // 2\n",
    "nBatchLoops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_training = time.time()\n",
    "\n",
    "total_epochs=1\n",
    "\n",
    "\n",
    "\n",
    "# inititalize loss weights\n",
    "weights = []\n",
    "weights_adv = []\n",
    "\n",
    "\n",
    "nEpochs = 5\n",
    "nSteps = nBatchLoops*nEpochs\n",
    "# nSteps = nEpochs // HPARAMS.trainingBatchSize\n",
    "\n",
    "print('Training for {0} epochs'.format(nEpochs))\n",
    "toPrint=True\n",
    "\n",
    "# train classifier\n",
    "# skipped = 0\n",
    "last_epoch=time.time()\n",
    "for epoch in range(nEpochs):\n",
    "\n",
    "    \n",
    "    if(toPrint):\n",
    "        print(\"\\nEpoch %d\" % (total_epochs,))\n",
    "        printTime(time.time() - last_epoch)\n",
    "        print('L_class', loss_class.numpy())\n",
    "        print('L_adv', loss_adv.numpy())\n",
    "        last_epoch = time.time()\n",
    "\n",
    "        total_epochs += 1\n",
    "#         toPrint=False\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    \n",
    "#     if(skipped > nBatchLoops-10): \n",
    "#         skipped = 0\n",
    "#         toPrint=True\n",
    "        \n",
    "    \n",
    "#     clipped_dataset = train_dataset.skip(skipped).take(HPARAMS.trainingBatchSize)\n",
    "#     clipped_dataset_adv = train_dataset_adv.skip(skipped).take(HPARAMS.trainingBatchSize)\n",
    "    \n",
    "    \n",
    "#     for i, (x, y) in enumerate(clipped_dataset):\n",
    "#         if(i==0): print(x['thetalab'][0].numpy())\n",
    "    \n",
    "#     skipped += HPARAMS.trainingBatchSize\n",
    "    \n",
    "    \n",
    "    # Iterate over the batches of the dataset.      \n",
    "    for step, (x_batch_train_adv, y_batch_train_adv) in enumerate(train_dataset_adv):\n",
    "\n",
    "        ### TRAIN ADVERSARY ###\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # logits for the batch\n",
    "            logits_adv = model_adv([classifier_output, x_batch_train_adv])\n",
    "\n",
    "            # calculate loss\n",
    "            loss_adv = loss_adv_fn(y_batch_train_adv, logits_adv)\n",
    "            \n",
    "            \n",
    "            adversary_weights = model_adv.trainable_weights\n",
    "            \n",
    "            # set adversary weights to 0\n",
    "            \n",
    "            #adversary_weights[0] = tf.Variable(adversary_weights[0].numpy()*[np.ones(64), np.zeros(64)], \n",
    "            #            name=adversary_weights[0].name, shape=adversary_weights[0].shape)\n",
    "            \n",
    "\n",
    "            # get and apply gradients\n",
    "            grads_adv = tape.gradient(loss_adv, model_adv.trainable_weights)\n",
    "            optimizer_adv.apply_gradients(zip(grads_adv, model_adv.trainable_weights))      \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "\n",
    "        ### TRAIN CLASSIFIER ###\n",
    "\n",
    "        # logits for the batch\n",
    "        logits_class = model_class(x_batch_train) \n",
    "\n",
    "        # calculate loss\n",
    "        loss_class = loss_class_fn(y_batch_train, logits_class)\n",
    "        \n",
    "        with tf.GradientTape() as tape_adv:\n",
    "            \n",
    "            adversary_weights = model_adv.trainable_weights\n",
    "            \n",
    "            # set adversary weights to 0\n",
    "            \n",
    "            #adversary_weights[0] = tf.Variable(adversary_weights[0].numpy()*[np.ones(64), np.zeros(64)], \n",
    "            #            name=adversary_weights[0].name, shape=adversary_weights[0].shape)\n",
    "            \n",
    "\n",
    "            # logits for the batch\n",
    "            logits_adv = model_adv([classifier_output, x_batch_train_adv])\n",
    "#             logits_adv = model_adv([classifier_output, x_batch_train_adv])\n",
    "\n",
    "\n",
    "\n",
    "            # calculate loss\n",
    "            loss_adv = loss_adv_fn(y_batch_train_adv, logits_adv)\n",
    "            \n",
    "\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape_class:\n",
    "\n",
    "            # logits for the batch\n",
    "            logits_class = model_class(x_batch_train) \n",
    "\n",
    "            \n",
    "            # get loss of classifier\n",
    "            loss_class = loss_class_fn(y_batch_train, logits_class)\n",
    "            \n",
    "            # get loss of adversary\n",
    "            loss_adv = loss_adv_fn(y_batch_train_adv, logits_adv)\n",
    "            \n",
    "     \n",
    "            \n",
    "            \n",
    "            tape_class.watch(logits_class)\n",
    "            tape_class.watch(y_batch_train)\n",
    "            tape_class.watch(logits_adv)\n",
    "            tape_class.watch(y_batch_train_adv)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # calculate total loss\n",
    "#             grads_total = calc_grad_total(model_class.trainable_weights, model_adv.trainable_weights)\n",
    "\n",
    "            \n",
    "            grads_total = calc_grad_total(logits_class, y_batch_train, logits_adv, y_batch_train_adv)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            # get gradients from classifier\n",
    "            grads_class_total = tape_class.gradient(grads_total, model_class.trainable_weights)            \n",
    "\n",
    "            \n",
    "            # apply gradients\n",
    "            optimizer_class.apply_gradients(zip(grads_class_total, model_class.trainable_weights))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            weights = grads_class\n",
    "            weights_adv = grads_adv\n",
    "            \n",
    "            \n",
    "            classifier_output = model_class(x_batch_train)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#         classifier_output = model_class(x_batch_train)\n",
    "\n",
    "\n",
    "end_training = time.time()\n",
    "print('Total time: ', end='')\n",
    "printTime(end_training - start_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model_adv = build_adversary()\n",
    "# model_adv.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), \n",
    "#                   loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "#                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_adv.fit(X_train['thetalab'], theta_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = {\n",
    "'theta':r'$\\Delta \\theta$',\n",
    "'pt':r'$\\mathrm{log}(p_{T})$',\n",
    "'eta':r'$\\eta$',\n",
    "'phi':r'$\\phi$',\n",
    "'mass':r'Mass',  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_avg = X_train['thetalab'].squeeze()\n",
    "pt_avg = np.average(X_train['pt'].squeeze(), axis=1)\n",
    "eta_avg = np.average(X_train['eta'].squeeze(), axis=1)\n",
    "phi_avg = np.average(X_train['phi'].squeeze(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins2d = [np.linspace(0,0.4,11),np.linspace(0,1,11)]\n",
    "vmin = 1e-1\n",
    "vmax=0.5*1e4\n",
    "norm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(theta_avg, predict_t2[:,1])\n",
    "# plt.xlabel(r'$\\Delta \\theta$')\n",
    "# # plt.xticks([2,3,4,5,6])\n",
    "# plt.ylabel(r'Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.average(X['pt'].squeeze(), axis=1), predict_t2[:,1])\n",
    "# plt.xlabel(r'$p_{T}$')\n",
    "# # plt.xticks([2,3,4,5,6])\n",
    "# plt.ylabel(r'Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = model_class.predict(X_train)\n",
    "predict_adv = model_adv.predict([predict, X_train['thetalab']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(15,7.5))\n",
    "\n",
    "\n",
    "ax[0][0].scatter(theta_avg, predict[:,1])\n",
    "ax[0][0].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[0][0].set_ylabel(r'Prediction')\n",
    "\n",
    "ax[0][1].scatter(pt_avg.flatten(), predict[:,1])\n",
    "ax[0][1].set_xlabel(xlabels['pt'])\n",
    "ax[0][1].set_ylabel(r'Prediction')\n",
    "\n",
    "ax[0][2].scatter(eta_avg, predict[:,1])\n",
    "ax[0][2].set_xlabel(xlabels['eta'])\n",
    "ax[0][2].set_ylabel(r'Prediction')\n",
    "\n",
    "ax[1][0].scatter(phi_avg, predict[:,1])\n",
    "ax[1][0].set_xlabel(xlabels['phi'])\n",
    "ax[1][0].set_ylabel(r'Prediction')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figname = 'Lambda'+str(HPARAMS.Lambda)+'_scatterplots.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_labels[:,2]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,3, figsize=(15,15))\n",
    "\n",
    "ax[0][0].scatter(theta_avg, predict_adv[:,2])\n",
    "ax[0][0].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[0][0].set_ylabel(r'Prediction')\n",
    "ax[0][0].set_title('Bin 3')\n",
    "ax[0][0].set_ylim([0,1])\n",
    "\n",
    "ax[0][1].scatter(theta_avg, predict_adv[:,3])\n",
    "ax[0][1].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[0][1].set_ylabel(r'Prediction')\n",
    "ax[0][1].set_title('Bin 4')\n",
    "ax[0][1].set_ylim([0,1])\n",
    "\n",
    "ax[0][2].scatter(theta_avg, predict_adv[:,4])\n",
    "ax[0][2].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[0][2].set_ylabel(r'Prediction')\n",
    "ax[0][2].set_title('Bin 5')\n",
    "ax[0][2].set_ylim([0,1])\n",
    "\n",
    "ax[1][0].scatter(theta_avg, predict_adv[:,5])\n",
    "ax[1][0].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[1][0].set_ylabel(r'Prediction')\n",
    "ax[1][0].set_title('Bin 6')\n",
    "ax[1][0].set_ylim([0,1])\n",
    "\n",
    "ax[1][1].scatter(theta_avg, predict_adv[:,6])\n",
    "ax[1][1].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[1][1].set_ylabel(r'Prediction')\n",
    "ax[1][1].set_title('Bin 7')\n",
    "ax[1][1].set_ylim([0,1])\n",
    "\n",
    "ax[1][2].scatter(theta_avg, predict_adv[:,7])\n",
    "ax[1][2].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[1][2].set_ylabel(r'Prediction')\n",
    "ax[1][2].set_title('Bin 8')\n",
    "ax[1][2].set_ylim([0,1])\n",
    "\n",
    "ax[2][0].scatter(theta_avg, predict_adv[:,8])\n",
    "ax[2][0].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[2][0].set_ylabel(r'Prediction')\n",
    "ax[2][0].set_title('Bin 9')\n",
    "ax[2][0].set_ylim([0,1])\n",
    "\n",
    "ax[2][1].scatter(theta_avg, predict_adv[:,9])\n",
    "ax[2][1].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[2][1].set_ylabel(r'Prediction')\n",
    "ax[2][1].set_title('Bin 10')\n",
    "ax[2][1].set_ylim([0,1])\n",
    "\n",
    "ax[2][2].scatter(theta_avg, predict_adv[:,10])\n",
    "ax[2][2].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[2][2].set_ylabel(r'Prediction')\n",
    "ax[2][2].set_title('Bin 11')\n",
    "ax[2][2].set_ylim([0,1])\n",
    "\n",
    "ax[3][0].scatter(theta_avg, predict_adv[:,0])\n",
    "ax[3][0].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[3][0].set_title('Bin 1')\n",
    "ax[3][0].set_ylabel(r'Prediction')\n",
    "\n",
    "ax[3][1].scatter(theta_avg, predict_adv[:,1])\n",
    "ax[3][1].set_xlabel(r'$\\Delta \\theta$')\n",
    "ax[3][1].set_title('Bin 2')\n",
    "ax[3][1].set_ylabel(r'Prediction')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "hist2d = plt.hist2d(theta_avg.flatten(), predict[:,1], norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$\\Delta \\theta$')\n",
    "# plt.xticks([2,3,4,5,6])\n",
    "plt.ylabel(r'Prediction')\n",
    "# plt.title('Second Training')\n",
    "plt.tight_layout()\n",
    "figname = 'Lambda'+str(HPARAMS.Lambda)+'_2D_theta.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "plt.show()\n",
    "\n",
    "vmin = np.min(hist2d[0].flatten()[np.abs(hist2d[0].flatten()) > 0])\n",
    "vmax = np.max(hist2d[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist2d(pt_avg.flatten(), predict[:,1], norm=LogNorm(), vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$p_{T}$')\n",
    "# plt.xticks([2,3,4,5,6])\n",
    "plt.ylabel(r'Prediction')\n",
    "# pl t.title('Second Training')\n",
    "plt.tight_layout()\n",
    "figname = 'Lambda'+str(HPARAMS.Lambda)+'_2D_pt.png'\n",
    "plt.savefig(figname)\n",
    "print('saving '+figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark down code for image\n",
    "\n",
    "<!-- ![](factor100_2.png) -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTime(time.time()-start_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
